{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Group 2\n",
    "Name: Akhilesh Yadav Gaddam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module D _(20 points)_\n",
    "\n",
    "__D1.__ _(7 points)_ Download and extract the `loan.csv` file from the [Lending Club Loan Dataset](https://www.kaggle.com/wendykan/lending-club-loan-data) and put it in the `data/` directory. Read the csv in (the first line contains headers). Create a dictionary named `statuses` whose keys are the entries in the `loan_status` and values are boolean values, describing 'good' or 'bad' loans, where loans that have \"Current\", \"Fully Paid\", or \"Issued\" in the `loan_status` field are 'good' loans, and others are 'bad' loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Charged Off': False,\n",
      " 'Current': True,\n",
      " 'Does not meet the credit policy. Status:Charged Off': False,\n",
      " 'Does not meet the credit policy. Status:Fully Paid': False,\n",
      " 'Fully Paid': True,\n",
      " 'In Grace Period': False,\n",
      " 'Late (16-30 days)': False,\n",
      " 'Late (31-120 days)': False}\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import pprint\n",
    "\n",
    "statuses = {}\n",
    "good_loan = []\n",
    "bad_loan = []\n",
    "\n",
    "def get_boolean(loan_status):\n",
    "    switch = {\n",
    "        \"Current\": True,\n",
    "        \"Fully Paid\": True,\n",
    "        \"Issued\": True\n",
    "    }\n",
    "    return switch.get(loan_status, False)\n",
    "  \n",
    "def get_statuses():\n",
    "        loan_csv=csv.reader(open(\"data/loan_small.csv\", \"r\"))\n",
    "        loan_list = list(loan_csv)\n",
    "        \n",
    "        for line in loan_list[1:]: \n",
    "            loan_status = line[16]\n",
    "    \n",
    "            statuses[loan_status] = get_boolean(loan_status)\n",
    "        \n",
    "            if statuses[loan_status] == False :\n",
    "                bad_loan.append(line)\n",
    "            else:\n",
    "                good_loan.append(line)\n",
    "                \n",
    "get_statuses()\n",
    "pprint.pprint(statuses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D2.__ _(8 pts)_\n",
    "The `desc` field contains text descriptions of loans. Tokenize each loan description and count the words for 'good' and 'bad' loan descriptions, putting counts into two separate `Counter()` data structures according to each loan's status in `statuses`. Print out the 50 most common words used to describe 'good' and 'bad' loans, respectively. \n",
    "\n",
    "__Note__: If you're still working on __D1__, for partial (near-full) credit you can still count all words, regardless of loan good/bad status, and print out the 50 most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 162402),\n",
      " ('on', 154004),\n",
      " ('I', 140284),\n",
      " ('added', 121096),\n",
      " ('>', 120785),\n",
      " ('Borrower', 119213),\n",
      " ('and', 110338),\n",
      " ('a', 100950),\n",
      " ('my', 95120),\n",
      " ('the', 83461),\n",
      " ('credit', 73561),\n",
      " ('for', 57274),\n",
      " ('of', 56170),\n",
      " ('have', 55600),\n",
      " ('pay', 52758),\n",
      " ('loan', 50780),\n",
      " ('in', 46066),\n",
      " ('off', 46034),\n",
      " ('is', 42895),\n",
      " ('will', 36893),\n",
      " ('debt', 34507),\n",
      " ('this', 33053),\n",
      " ('interest', 30027),\n",
      " ('with', 29701),\n",
      " ('card', 29390),\n",
      " ('be', 28634),\n",
      " ('am', 27830),\n",
      " ('that', 26427),\n",
      " ('cards', 23785),\n",
      " ('consolidate', 21928),\n",
      " ('one', 18142),\n",
      " ('would', 17967),\n",
      " ('This', 17568),\n",
      " ('me', 17076),\n",
      " ('payment', 16044),\n",
      " ('all', 15868),\n",
      " ('high', 15428),\n",
      " ('at', 14943),\n",
      " ('been', 14478),\n",
      " ('as', 14464),\n",
      " ('monthly', 14035),\n",
      " ('like', 13588),\n",
      " ('get', 13513),\n",
      " ('it', 13278),\n",
      " ('some', 12914),\n",
      " ('paying', 12304),\n",
      " ('so', 12302),\n",
      " ('My', 11885),\n",
      " ('are', 11075),\n",
      " ('you', 10900)]\n",
      "[('to', 35567),\n",
      " ('I', 31462),\n",
      " ('on', 31169),\n",
      " ('and', 25389),\n",
      " ('added', 24125),\n",
      " ('>', 24052),\n",
      " ('Borrower', 23633),\n",
      " ('my', 21685),\n",
      " ('a', 21238),\n",
      " ('the', 18070),\n",
      " ('credit', 14165),\n",
      " ('for', 12785),\n",
      " ('of', 12522),\n",
      " ('have', 12178),\n",
      " ('pay', 11092),\n",
      " ('in', 10150),\n",
      " ('loan', 10094),\n",
      " ('is', 9117),\n",
      " ('off', 9026),\n",
      " ('will', 7975),\n",
      " ('this', 7146),\n",
      " ('be', 6396),\n",
      " ('with', 6347),\n",
      " ('debt', 6295),\n",
      " ('am', 6229),\n",
      " ('that', 5921),\n",
      " ('card', 4995),\n",
      " ('cards', 4869),\n",
      " ('interest', 4719),\n",
      " ('consolidate', 4459),\n",
      " ('would', 4307),\n",
      " ('one', 4206),\n",
      " ('me', 3898),\n",
      " ('all', 3780),\n",
      " ('as', 3482),\n",
      " ('payment', 3394),\n",
      " ('This', 3273),\n",
      " ('get', 3219),\n",
      " ('some', 3159),\n",
      " ('monthly', 3069),\n",
      " ('like', 3010),\n",
      " ('been', 2936),\n",
      " ('it', 2928),\n",
      " ('at', 2923),\n",
      " ('you', 2797),\n",
      " ('so', 2761),\n",
      " ('high', 2738),\n",
      " ('help', 2692),\n",
      " ('need', 2674),\n",
      " ('My', 2539)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from collections import Counter\n",
    "good_word = Counter()\n",
    "bad_word = Counter()\n",
    "\n",
    "def tokenize(loan, counter):\n",
    "    for line in loan:\n",
    "        desc = line[19]\n",
    "        \n",
    "        word = desc.strip().split()\n",
    "        counter.update(word)\n",
    "    \n",
    "tokenize(good_loan, good_word)\n",
    "tokenize(bad_loan, bad_word)\n",
    "\n",
    "pprint.pprint(good_word.most_common(50))\n",
    "pprint.pprint(bad_word.most_common(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D3.__ _(5 points)_ Discuss your output and the choice of tokenization that you used in the response box below. Do the two different highly-frequeny word printouts make sense? What impacts do you think your choice had on the output? What were the challenges and what could have worked better or worse?\n",
    "\n",
    "__Note__: even if you are still working on __D1__ you can count words as discussed in __D2__ and. comment on your choices for tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Afer looking at the speed at which the immense dataset was parsed, I am truly sure about the decision of \n",
    "the tokenization technique I have picked. The counter data structure has helped me a lot. The decision of \n",
    "tokenization will assist us to index and read the data quickly as compared with different strategies.\n",
    "Yes, the two different highly-frequeny word printouts make sense. The challenging part was reading such a huge \n",
    "data and tokenizing the data, as there would be complexity issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
